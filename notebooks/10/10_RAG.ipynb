{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ed89688",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/hussamalafandi/Generative_AI/blob/main/notebooks/10/10_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3daaf42",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG): Enhancing LLMs with External Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad833a47",
   "metadata": {},
   "source": [
    "## Introduction to Retrieval-Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5243b68d",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"rag_flow.png\" alt=\"RAG Flow Diagram\" style=\"width: 60%; height: auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e421a57",
   "metadata": {},
   "source": [
    "## What is Retrieval-Augmented Generation (RAG)?\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is a technique used to enhance large language models (LLMs) by integrating external knowledge retrieved from document databases or knowledge stores. Unlike conventional generative models, which rely solely on learned parameters from training data, RAG dynamically accesses up-to-date and contextually relevant information, significantly improving the accuracy, reliability, and usefulness of the generated responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc09bd27",
   "metadata": {},
   "source": [
    "The core idea behind RAG is simple yet powerful:\n",
    "\n",
    "- **Retrieve**: When a user provides a query or prompt, RAG first retrieves relevant documents or passages from an external knowledge base.\n",
    "\n",
    "- **Generate**: The model then uses the retrieved documents as context to generate accurate, informed, and detailed responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c7ada5",
   "metadata": {},
   "source": [
    "### Why Retrieval Matters in Generative AI?\n",
    "\n",
    "Retrieval methods address fundamental limitations of purely parametric generative models:\n",
    "\n",
    "* **Factual Accuracy**: Retrieval enables models to access the latest and accurate data rather than relying solely on outdated training datasets.\n",
    "* **Reducing Hallucinations**: By grounding generation in retrieved information, RAG significantly reduces the chances of generating incorrect, nonsensical, or fabricated information.\n",
    "* **Scalability**: Retrieval allows LLMs to leverage large-scale, dynamic knowledge bases efficiently without retraining the entire model when information updates occur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f692f38",
   "metadata": {},
   "source": [
    "### Limitations of Traditional LLMs:\n",
    "\n",
    "Traditional language models have some well-known drawbacks:\n",
    "\n",
    "* **Hallucination**: Generating plausible but incorrect or unsupported information.\n",
    "* **Stale Knowledge**: Limited to static training data, lacking awareness of recent updates or newly available information.\n",
    "* **Context Limitations**: Without retrieval, LLMs have fixed-size context windows, severely limiting their ability to reference extensive external knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a445ec3",
   "metadata": {},
   "source": [
    "### Real-world Examples and Use Cases\n",
    "\n",
    "**Knowledge-base Q&A Systems**\n",
    "* Quickly answering user questions by retrieving precise, authoritative information from structured or unstructured sources.\n",
    "* Example: Customer support systems retrieving relevant FAQ or product manuals to answer customer queries.\n",
    "\n",
    "**Chatbots with External Knowledge Bases**\n",
    "* Dynamic chatbots integrated with knowledge bases or external databases to offer up-to-date, personalized interactions.\n",
    "* Example: Travel assistant chatbot retrieving flight schedules, weather data, and travel restrictions.\n",
    "\n",
    "**Enterprise-level AI Assistants**\n",
    "* Assisting professionals in fields such as law, medicine, or technical documentation by providing quick access to domain-specific knowledge.\n",
    "* Example: Medical assistants that generate treatment suggestions based on the latest clinical guidelines and patient histories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8fefb9",
   "metadata": {},
   "source": [
    "## Core Concepts and Components of RAG\n",
    "\n",
    "To effectively build and deploy Retrieval-Augmented Generation systems, itâ€™s crucial to understand their core components: the **Retriever**, the **Generator (Reader)**, and the overall **End-to-End Flow**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe900098",
   "metadata": {},
   "source": [
    "### Retriever\n",
    "\n",
    "The retriever component is responsible for identifying and fetching the most relevant documents or information chunks from an external knowledge base given a query. Retrieval methods typically fall into two categories: **Sparse** and **Dense**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cf04eb",
   "metadata": {},
   "source": [
    "##### Sparse Methods (Keyword-Based):\n",
    "\n",
    "Sparse retrieval methods rely on exact term matches and statistical weighting (like TF-IDF or BM25).\n",
    "\n",
    "* **TF-IDF**: Scores words based on frequency across documents.\n",
    "* **BM25**: An improvement that adjusts for document length and term saturation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53fa875f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary terms:\n",
      "\n",
      "['and' 'are' 'cat' 'cats' 'companions' 'dogs' 'lovely' 'mat' 'on' 'pets'\n",
      " 'red' 'sat' 'soft' 'the' 'was']\n",
      "\n",
      "TF-IDF Weighted Document-Term Matrix:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>are</th>\n",
       "      <th>cat</th>\n",
       "      <th>cats</th>\n",
       "      <th>companions</th>\n",
       "      <th>dogs</th>\n",
       "      <th>lovely</th>\n",
       "      <th>mat</th>\n",
       "      <th>on</th>\n",
       "      <th>pets</th>\n",
       "      <th>red</th>\n",
       "      <th>sat</th>\n",
       "      <th>soft</th>\n",
       "      <th>the</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.319</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.401</td>\n",
       "      <td>0.401</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.401</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.357</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     and    are    cat   cats  companions   dogs  lovely    mat     on   pets  \\\n",
       "0  0.000  0.000  0.405  0.000       0.000  0.000   0.000  0.319  0.405  0.000   \n",
       "1  0.401  0.401  0.000  0.509       0.000  0.509   0.000  0.000  0.000  0.401   \n",
       "2  0.357  0.000  0.000  0.000       0.000  0.000   0.000  0.357  0.000  0.000   \n",
       "3  0.000  0.438  0.000  0.000       0.555  0.000   0.555  0.000  0.000  0.438   \n",
       "\n",
       "     red    sat   soft    the    was  \n",
       "0  0.000  0.405  0.000  0.638  0.000  \n",
       "1  0.000  0.000  0.000  0.000  0.000  \n",
       "2  0.453  0.000  0.453  0.357  0.453  \n",
       "3  0.000  0.000  0.000  0.000  0.000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample documents\n",
    "docs = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"Dogs and cats are pets.\",\n",
    "    \"The mat was red and soft.\",\n",
    "    \"Pets are lovely companions.\"\n",
    "]\n",
    "\n",
    "# Step 1: Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Step 2: Fit and transform the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(docs)\n",
    "\n",
    "# Step 3: Get the list of terms (features)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Step 4: Convert the TF-IDF matrix into a DataFrame for better readability\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=terms)\n",
    "\n",
    "# Step 5: Display the terms\n",
    "print(\"Vocabulary terms:\\n\")\n",
    "print(terms)\n",
    "\n",
    "# Step 6: Display the TF-IDF matrix nicely\n",
    "print(\"\\nTF-IDF Weighted Document-Term Matrix:\\n\")\n",
    "tfidf_df.round(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a860b6de",
   "metadata": {},
   "source": [
    "##### **Exercise**:\n",
    "Modify the above code to search which document is most relevant to the query: \"cats love mats\".\n",
    "(Hint: Vectorize the query and compute cosine similarity!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156de235",
   "metadata": {},
   "source": [
    "##### Dense Retrieval (Embeddings)\n",
    "\n",
    "Dense retrieval methods use vector embeddings to capture semantic similarity rather than exact keyword matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b73bc892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar document to 'A soft mat for pets': The cat sat on the mat.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Create document embeddings\n",
    "doc_embeddings = model.encode(docs, convert_to_tensor=True)\n",
    "\n",
    "# Query\n",
    "query = \"A soft mat for pets\"\n",
    "query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "# Compute similarity\n",
    "cos_scores = util.cos_sim(query_embedding, doc_embeddings)\n",
    "\n",
    "# Find the most similar document\n",
    "most_similar_idx = cos_scores.argmax()\n",
    "print(f\"Most similar document to '{query}': {docs[most_similar_idx]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18890b0",
   "metadata": {},
   "source": [
    "##### **Exercise**\n",
    "\n",
    "Try a different query like \"Companions for humans\" and check which document ranks highest!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef95ad6b",
   "metadata": {},
   "source": [
    "### Vector Database\n",
    "\n",
    "Vector databases efficiently store, manage, and retrieve dense embeddings at scale. They are critical in modern RAG implementations.\n",
    "\n",
    "Popular options:\n",
    "\n",
    "* **FAISS** (open-source, very fast for local)\n",
    "* **ChromaDB** (easy for prototyping)\n",
    "* **Pinecone** (scalable, cloud-based)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f4030e",
   "metadata": {},
   "source": [
    "> To run the next code cells you need to [install faiss](https://github.com/facebookresearch/faiss/blob/main/INSTALL.md). (`pip install faiss-cpu`) and ChromaDB (`pip install chromadb`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1733b91",
   "metadata": {},
   "source": [
    "We will use FAISS to build a fast in-memory index of document embeddings and perform a similarity search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f009af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to document using FAISS: The cat sat on the mat.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Convert embeddings to numpy\n",
    "doc_embeddings_np = doc_embeddings.cpu().detach().numpy()\n",
    "\n",
    "# Build FAISS index\n",
    "dimension = doc_embeddings_np.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(doc_embeddings_np)\n",
    "\n",
    "# Search with query\n",
    "query_np = query_embedding.cpu().detach().numpy().reshape(1, -1)\n",
    "distances, indices = index.search(query_np, k=1)\n",
    "\n",
    "print(f\"Most similar to document using FAISS: {docs[indices[0][0]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5815ef1",
   "metadata": {},
   "source": [
    "Now, we'll use ChromaDB to store both embeddings and documents, and perform a semantic search with document retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f110c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar document using ChromaDB: The cat sat on the mat.\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# Create a collection (like an index in FAISS)\n",
    "collection = chroma_client.create_collection(name=\"my_collection\")\n",
    "\n",
    "# Add documents and embeddings\n",
    "collection.add(\n",
    "    embeddings=doc_embeddings_np.tolist(),  # convert numpy to list\n",
    "    documents=[doc for doc in docs],         # documents list\n",
    "    ids=[str(i) for i in range(len(docs))]    # unique string IDs\n",
    ")\n",
    "\n",
    "# Query ChromaDB\n",
    "results = collection.query(\n",
    "    query_embeddings=query_np.tolist(),  # query as a list\n",
    "    n_results=1\n",
    ")\n",
    "\n",
    "print(f\"Most similar document using ChromaDB: {results['documents'][0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867f40fb",
   "metadata": {},
   "source": [
    "##### Key Differences: **FAISS** vs **ChromaDB**\n",
    "\n",
    "* **FAISS** stores only **embeddings**, while **ChromaDB** stores **both embeddings and documents**.\n",
    "* With **FAISS**, you work with implicit numeric indices; **ChromaDB** requires you to provide **document IDs**.\n",
    "* **FAISS** returns distances and indices; **ChromaDB** directly returns the **matching documents** along with scores.\n",
    "* **ChromaDB** also supports **persistence** out of the box, making it easier for saving and reloading collections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1654d8e5",
   "metadata": {},
   "source": [
    "> **Note:**  \n",
    "> This notebook shows minimal FAISS and ChromaDB usage for RAG systems. For production, consider proper persistence and indexing configurations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecee8fce",
   "metadata": {},
   "source": [
    "##### **Exercise**\n",
    "\n",
    "Index more documents and retrieve the top 3 most similar documents instead of just 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8053a53f",
   "metadata": {},
   "source": [
    "### Reader/Generator\n",
    "\n",
    "After retrieval, the generator (also called the reader or the generative component) synthesizes the retrieved information into a coherent, relevant answer or output.\n",
    "\n",
    "It utilizes the retrieved documents as context within the model's input prompts, enabling responses grounded firmly in factual information rather than relying solely on internal knowledge from training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9928bb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: The cat sat on the mat.\n",
      "\n",
      "Question: Where do cats usually sit?\n",
      "\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "# Retrieved document\n",
    "context = results['documents'][0][0]\n",
    "\n",
    "# User question\n",
    "question = \"Where do cats usually sit?\"\n",
    "\n",
    "# Simple prompt\n",
    "prompt = f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19687996",
   "metadata": {},
   "source": [
    "Now we pass the prompt to an LLM to generate an answer grounded in the retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f68e97cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: The cat sat on the mat.\n",
      "\n",
      "Question: Where do cats usually sit?\n",
      "\n",
      "Answer: Mats.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Assuming you have a text generation pipeline ready with Gemma 3\n",
    "generator = pipeline(\"text-generation\", model=\"google/gemma-3-1b-it\", device='cpu')\n",
    "\n",
    "# Generate an answer\n",
    "response = generator(prompt, max_new_tokens=100, do_sample=True)\n",
    "\n",
    "# Print the generated answer\n",
    "print(response[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35e8c18",
   "metadata": {},
   "source": [
    "##### **Exercise**\n",
    "\n",
    "Rewrite the prompt to:\n",
    "\n",
    "- Instruct the model to only answer using the provided context.\n",
    "- Tell the model to say \"I don't know\" if the answer is missing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6419852a",
   "metadata": {},
   "source": [
    "# Additional Resources\n",
    "\n",
    "* LangChain Conceptual Guides [RAG](https://python.langchain.com/docs/concepts/rag/) and [RAG From Scratch](https://github.com/langchain-ai/rag-from-scratch)\n",
    "* LangChain Tutorial on RAG [Part 1](https://python.langchain.com/docs/tutorials/rag/) and [Part 2](https://python.langchain.com/docs/tutorials/qa_chat_history/)\n",
    "* WandB Course [RAG++ : From POC to Production](https://wandb.ai/site/courses/rag)\n",
    "* LangChain [Multi-Vector Retriever](https://blog.langchain.dev/semi-structured-multi-modal-rag/)\n",
    "* [How to pass multimodal data to models](https://python.langchain.com/docs/how_to/multimodal_inputs/)\n",
    "* [Chroma multi-modal RAG](https://github.com/langchain-ai/langchain/blob/master/cookbook/multi_modal_RAG_chroma.ipynb)\n",
    "* [pdf-retrieval-with-ColQwen2-vlm_Vespa-cloud](https://colab.research.google.com/github/vespa-engine/pyvespa/blob/master/docs/sphinx/source/examples/pdf-retrieval-with-ColQwen2-vlm_Vespa-cloud.ipynb#scrollTo=PUqnrKWLak3O)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
