{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hussamalafandi/Generative_AI/blob/main/notebooks/08/08_training_and_fine-tuning_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05f170a6",
      "metadata": {
        "id": "05f170a6"
      },
      "source": [
        "# From Pre-training to Fine-tuning: Practical Guide for Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fd4b8d7",
      "metadata": {
        "id": "9fd4b8d7"
      },
      "source": [
        "## Learning Objectives:\n",
        "\n",
        "By the end of this notebook, students will:\n",
        "\n",
        "* Understand the differences and purposes of pre-training, supervised fine-tuning (SFT), and preference-based training (DPO).\n",
        "* Be able to fine-tune small, efficient transformer models (SmolLM2-135M) on practical datasets.\n",
        "* Evaluate fine-tuned models quantitatively (perplexity/loss) and qualitatively (generation quality).\n",
        "* Get introduced to high-performance libraries (Unsloth) for efficient transformer fine-tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1819a07",
      "metadata": {
        "id": "d1819a07"
      },
      "source": [
        "## Introduction and Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "644c1442",
      "metadata": {
        "id": "644c1442"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"https://images.ctfassets.net/kftzwdyauwt9/40in10B8KtAGrQvwRv5cop/8241bb17c283dced48ea034a41d7464a/chatgpt_diagram_light.png?w=2048&q=80&fm=webp\" alt=\"ChatGPT training phases\" width=\"1200\" />\n",
        "</div>\n",
        "\n",
        "Image source: [openai.com](https://openai.com/index/chatgpt/)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51c7c937",
      "metadata": {
        "id": "51c7c937"
      },
      "source": [
        "### What is Pre-training?\n",
        "\n",
        "**Pre-training** is a process in which language models are initially trained on large-scale datasets using **self-supervised learning**. In simple terms, these models learn directly from the text itself without explicit labels provided by humans.\n",
        "\n",
        "Common pre-training tasks include:\n",
        "\n",
        "* **Causal Language Modeling (CLM):** Predicting the next word/token based on previous context.\n",
        "\n",
        "* **Masked Language Modeling (MLM):** Predicting hidden (masked) words in a sentence (e.g., used by BERT).\n",
        "\n",
        "Through pre-training, models capture fundamental language understanding, grammar, reasoning, and context comprehension. This general knowledge becomes the basis for further specialization through fine-tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "868b80ab",
      "metadata": {
        "id": "868b80ab"
      },
      "source": [
        "#### SmolLM2 (Small and Efficient Model)\n",
        "\n",
        "In this course, we’ll use SmolLM2-135M, a compact and efficient transformer-based language model developed specifically to offer robust performance on modest hardware resources. SmolLM2 balances capability and efficiency, making it ideal for educational purposes, prototyping, and running on limited hardware (e.g., GPUs available via Colab)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "628f28f8",
      "metadata": {
        "id": "628f28f8"
      },
      "source": [
        "### What is Supervised Fine-Tuning (SFT)?\n",
        "\n",
        "Although pre-trained models have broad language capabilities, they often lack task-specific accuracy. **Supervised Fine-Tuning (SFT)** bridges this gap, refining the general knowledge learned during pre-training by training the model further on task-specific data with explicit labels or prompts.\n",
        "\n",
        "Through fine-tuning, models become adept at specialized tasks, such as:\n",
        "\n",
        "* Conversational assistants\n",
        "* Text summarization\n",
        "* Sentiment analysis\n",
        "* Domain-specific language generation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbeca4e5",
      "metadata": {
        "id": "cbeca4e5"
      },
      "source": [
        "#### SmolTalk Dataset\n",
        "\n",
        "In this notebook, we'll practically perform supervised fine-tuning using **SmolTalk**, a compact conversational dataset designed specifically for fine-tuning lightweight language models. SmolTalk is carefully crafted to provide realistic conversational exchanges, helping our SmolLM2 model improve significantly in generating natural dialogues and responding to instructions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30f76cdf",
      "metadata": {
        "id": "30f76cdf"
      },
      "source": [
        "### Preference-based Fine-tuning (DPO)\n",
        "\n",
        "Beyond supervised fine-tuning, recent approaches also incorporate **Preference-based Fine-tuning (DPO, Direct Preference Optimization)**. Instead of optimizing purely for task accuracy or next-token prediction, DPO fine-tunes models based on human-generated feedback indicating preference for specific responses.\n",
        "\n",
        "This method ensures models not only become task-specific but also align better with human preferences, improving their real-world usability, helpfulness, and alignment with ethical guidelines."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8943eee0",
      "metadata": {
        "id": "8943eee0"
      },
      "source": [
        "#### UltraFeedback Dataset (Introduction)\n",
        "\n",
        "Later in our course, we will explore DPO practically using the **UltraFeedback dataset**, which contains numerous examples of human preferences ranking model-generated outputs. UltraFeedback helps models like SmolLM2 adapt to human-preferred conversational styles and improves their ability to generate helpful, appropriate, and aligned responses."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f69a4987",
      "metadata": {
        "id": "f69a4987"
      },
      "source": [
        "\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/RvHjdlRT5gGQt5mJuhXH9.png\" alt=\"SmolLM2 Ecosystem\" width=\"1200\" />\n",
        "</div>\n",
        "\n",
        "Image source: [huggingface.co](https://huggingface.co/HuggingFaceTB)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f64815b",
      "metadata": {
        "id": "8f64815b"
      },
      "source": [
        "## Setup and Dataset Preparation\n",
        "\n",
        "In this section, you'll set up your environment by installing essential libraries and exploring the dataset (**SmolTalk**) we'll use for supervised fine-tuning (SFT)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets trl torch torchvision huggingface_hub evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-50XK7QHQN6e",
        "outputId": "93ad14c4-c707-45ae-a2f1-fc441c5ab8e2"
      },
      "id": "-50XK7QHQN6e",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.11/dist-packages (0.16.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.30.2)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from trl) (1.5.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl) (13.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (5.9.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d03d68b7",
      "metadata": {
        "id": "d03d68b7"
      },
      "source": [
        "Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b7c273e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7c273e2",
        "outputId": "ba43a5fb-4dd5-466d-e562-175eb905719e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'content': \"You're an AI assistant for text re-writing. Rewrite the input \"\n",
            "             'text to make it more concise while preserving its core meaning.',\n",
            "  'role': 'system'},\n",
            " {'content': 'Hey Alex,\\n'\n",
            "             '\\n'\n",
            "             \"I hope you're doing well! It's been a while since we met at the \"\n",
            "             'film festival last year. I was the one with the short film about '\n",
            "             \"the old abandoned factory. Anyway, I'm reaching out because I'm \"\n",
            "             'currently working on my thesis film project and I could really '\n",
            "             'use some advice on cinematography. I remember our conversation '\n",
            "             'about visual storytelling and I was hoping you might have some '\n",
            "             'tips or insights to share.\\n'\n",
            "             '\\n'\n",
            "             'My film is a drama set in a small town, and I want to capture '\n",
            "             'the mood and atmosphere of the location through my '\n",
            "             \"cinematography. I'm planning to shoot on location next month, \"\n",
            "             \"but I'm still trying to figure out the best way to approach it. \"\n",
            "             'If you have any suggestions or resources you could point me to, '\n",
            "             'I would be incredibly grateful.\\n'\n",
            "             '\\n'\n",
            "             \"Also, I heard from a mutual friend that you're having a \"\n",
            "             'photography exhibition soon. Congratulations! I would love to '\n",
            "             \"attend if you don't mind sending me the details.\\n\"\n",
            "             '\\n'\n",
            "             'Thanks in advance for any help you can provide. I really '\n",
            "             'appreciate it.\\n'\n",
            "             '\\n'\n",
            "             'Best,\\n'\n",
            "             'Jordan',\n",
            "  'role': 'user'},\n",
            " {'content': 'Hey Alex,\\n'\n",
            "             '\\n'\n",
            "             \"Hope you're well! It's Jordan from the film festival last year. \"\n",
            "             \"I'm working on my thesis film, a drama set in a small town, and \"\n",
            "             'could use your advice on cinematography to capture the mood and '\n",
            "             'atmosphere. Any tips or resources would be great.\\n'\n",
            "             '\\n'\n",
            "             \"Also, congrats on your upcoming photography exhibition! I'd love \"\n",
            "             'to attend; could you send me the details?\\n'\n",
            "             '\\n'\n",
            "             'Thanks a lot!\\n'\n",
            "             '\\n'\n",
            "             'Best,\\n'\n",
            "             'Jordan',\n",
            "  'role': 'assistant'}]\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from pprint import pprint\n",
        "\n",
        "train_dataset = load_dataset(\"HuggingFaceTB/smoltalk\", \"smol-rewrite\", split=\"train\")\n",
        "eval_dataset = load_dataset(\"HuggingFaceTB/smoltalk\", \"smol-rewrite\", split=\"test\")\n",
        "\n",
        "pprint(train_dataset[0]['messages'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "845d904f",
      "metadata": {
        "id": "845d904f"
      },
      "source": [
        "Load **SmolLM2** Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8c81e241",
      "metadata": {
        "id": "8c81e241"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M-Instruct\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d9c80e5",
      "metadata": {
        "id": "6d9c80e5"
      },
      "source": [
        "The loaded dataset contains conversations with distinct roles, which need to be formatted into a consistent text sequence before being fed into the language model.\n",
        "\n",
        "A **chat template** defines the structure for this formatting, ensuring uniformity in how roles are represented. Many tokenizers, including the SmolLM2 tokenizer, provide built-in methods for applying such templates.\n",
        "\n",
        "The SmolLM2 tokenizer includes a chat template specifically designed to align with its conversational style."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f6be5c39",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6be5c39",
        "outputId": "3ef1cc2b-5ca1-48c4-a7ee-ea6224496cfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Formatted input with chat template:\n",
            " <|im_start|>system\n",
            "You're an AI assistant for text re-writing. Rewrite the input text to make it more concise while preserving its core meaning.<|im_end|>\n",
            "<|im_start|>user\n",
            "Hey Alex,\n",
            "\n",
            "I hope you're doing well! It's been a while since we met at the film festival last year. I was the one with the short film about the old abandoned factory. Anyway, I'm reaching out because I'm currently working on my thesis film project and I could really use some advice on cinematography. I remember our conversation about visual storytelling and I was hoping you might have some tips or insights to share.\n",
            "\n",
            "My film is a drama set in a small town, and I want to capture the mood and atmosphere of the location through my cinematography. I'm planning to shoot on location next month, but I'm still trying to figure out the best way to approach it. If you have any suggestions or resources you could point me to, I would be incredibly grateful.\n",
            "\n",
            "Also, I heard from a mutual friend that you're having a photography exhibition soon. Congratulations! I would love to attend if you don't mind sending me the details.\n",
            "\n",
            "Thanks in advance for any help you can provide. I really appreciate it.\n",
            "\n",
            "Best,\n",
            "Jordan<|im_end|>\n",
            "<|im_start|>assistant\n",
            "Hey Alex,\n",
            "\n",
            "Hope you're well! It's Jordan from the film festival last year. I'm working on my thesis film, a drama set in a small town, and could use your advice on cinematography to capture the mood and atmosphere. Any tips or resources would be great.\n",
            "\n",
            "Also, congrats on your upcoming photography exhibition! I'd love to attend; could you send me the details?\n",
            "\n",
            "Thanks a lot!\n",
            "\n",
            "Best,\n",
            "Jordan<|im_end|>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Using tokenizer's apply_chat_template method (if provided)\n",
        "# We'll format the example as an instruction-response pair\n",
        "chat_example = train_dataset[0]['messages']\n",
        "\n",
        "# Check the default chat template (optional, but useful for inspection)\n",
        "formatted_input = tokenizer.apply_chat_template(chat_example, tokenize=False)\n",
        "print(\"\\nFormatted input with chat template:\\n\", formatted_input)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5d9e85d",
      "metadata": {
        "id": "d5d9e85d"
      },
      "source": [
        "Now, let's tokenize our dataset efficiently using the chat template. We'll define a tokenization function and apply it to the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7d348eb5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405,
          "referenced_widgets": [
            "0d225f4df3194a859266bf18c80176a5",
            "68bb44b537874f38a05c4e12bf25e628",
            "f10618b640554bdbbce4edc2ae43b8a5",
            "101d870bf3f740abb8e966d0519caf60",
            "359a7a4fc0684f26aade5d8ca598f21c",
            "552f806a85cf4b59805f41c179d294e1",
            "2d0bd3532f944acfa475058d676f80a1",
            "545cc548357b44fe8386d974d789f55f",
            "78ece4b4d7fc4796bb9846164a833a7c",
            "7547452333bb4b9c971a2415e9f925c6",
            "87a95d64d85e4ab8b697239e0fc79a6b"
          ]
        },
        "id": "7d348eb5",
        "outputId": "906778ae-0444-4177-d680-897a022a4abb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/53342 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d225f4df3194a859266bf18c80176a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-bd3a23a3a0c0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Tokenize the entire dataset using the custom function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m tokenized_dataset = train_dataset.map(\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtokenize_with_chat_template\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m         }\n\u001b[1;32m    556\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3072\u001b[0m                     \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"Map\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3073\u001b[0m                 ) as pbar:\n\u001b[0;32m-> 3074\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdataset_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3075\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3076\u001b[0m                             \u001b[0mshards_done\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3529\u001b[0m                                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_arrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3530\u001b[0m                             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3531\u001b[0;31m                                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3532\u001b[0m                         \u001b[0mnum_examples_progress_update\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnum_examples_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3533\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_time\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPBAR_REFRESH_TIME_INTERVAL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_writer.py\u001b[0m in \u001b[0;36mwrite_batch\u001b[0;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_rows_on_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m     def write_batch(\n\u001b[0m\u001b[1;32m    570\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0mbatch_examples\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def tokenize_with_chat_template(example):\n",
        "    # Use the tokenizer's apply_chat_template method to format the input\n",
        "    formatted_input = tokenizer.apply_chat_template(example['messages'], tokenize=False)\n",
        "\n",
        "    # Tokenize the formatted input\n",
        "    tokenized_input = tokenizer(formatted_input, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    return tokenized_input\n",
        "\n",
        "# Tokenize the entire dataset using the custom function\n",
        "tokenized_dataset = train_dataset.map(\n",
        "    tokenize_with_chat_template,\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5eb73c8d",
      "metadata": {
        "id": "5eb73c8d"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a4ecad7",
      "metadata": {
        "id": "5a4ecad7"
      },
      "outputs": [],
      "source": [
        "print(tokenized_dataset[0]['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35fbdf6d",
      "metadata": {
        "id": "35fbdf6d"
      },
      "outputs": [],
      "source": [
        "# Decode back for sanity check\n",
        "decoded_sample = tokenizer.decode(tokenized_dataset[0]['input_ids'])\n",
        "print(\"\\nDecoded sample:\\n\", decoded_sample)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "bd5f6ea5",
      "metadata": {
        "id": "bd5f6ea5"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert_score sentence_transformers evaluate scikit-learn rouge_score"
      ],
      "metadata": {
        "id": "qDLySvwPgv_r"
      },
      "id": "qDLySvwPgv_r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "import bert_score\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load necessary models and metrics\n",
        "# metric_bleu = evaluate.load(\"bleu\")\n",
        "# metric_rouge = evaluate.load(\"rouge\")\n",
        "# sentence_model = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # Decode predictions and labels to text\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Calculate BLEU and ROUGE\n",
        "    # bleu_result = metric_bleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    # rouge_result = metric_rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "\n",
        "    # Calculate BERTScore\n",
        "    P, R, F1 = bert_score.score(decoded_preds, decoded_labels, model_type=\"bert-base-uncased\")\n",
        "\n",
        "    # Calculate cosine similarity using sentence embeddings\n",
        "    # original_embeddings = sentence_model.encode(decoded_labels)\n",
        "    # rewritten_embeddings = sentence_model.encode(decoded_preds)\n",
        "    # similarity_scores = cosine_similarity(original_embeddings, rewritten_embeddings)\n",
        "\n",
        "    # Combine results into a dictionary\n",
        "    result = {\n",
        "        # \"bleu\": bleu_result[\"bleu\"],\n",
        "        # \"rouge1\": rouge_result[\"rouge1\"],\n",
        "        # \"rouge2\": rouge_result[\"rouge2\"],\n",
        "        # \"rougeL\": rouge_result[\"rougeL\"],\n",
        "        # \"rougeLsum\": rouge_result[\"rougeLsum\"],\n",
        "        \"bertscore\": F1.mean().item(),\n",
        "        # \"avg_cosine_similarity\": similarity_scores.diagonal().mean()\n",
        "    }\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "NEqCytL-gjMX"
      },
      "id": "NEqCytL-gjMX",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "3f70b9ee",
      "metadata": {
        "id": "3f70b9ee"
      },
      "outputs": [],
      "source": [
        "training_args = SFTConfig(\n",
        "    output_dir=\"./smollm2-sft-results\",   # Output directory for model checkpoints\n",
        "    num_train_epochs=3,                   # Number of epochs\n",
        "    learning_rate=3e-5,                   # Learning rate # try with 5e-5\n",
        "    logging_steps=10,                     # How often to log training progress\n",
        "    save_steps=100,                       # How often to save checkpoints\n",
        "    per_device_train_batch_size=4,        # Set according to your GPU memory capacity\n",
        "    per_device_eval_batch_size=8,\n",
        "    eval_strategy=\"steps\",                # Evaluate the model at regular intervals\n",
        "    eval_steps=50,                        # Frequency of evaluation\n",
        "    # gradient_accumulation_steps=4,\n",
        "    # gradient_checkpointing=True,\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4da680f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "4da680f6",
        "outputId": "cc5b33f0-a848-49de-cdc4-7a567713ee1c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='251' max='40008' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  251/40008 14:34 < 38:47:25, 0.28 it/s, Epoch 0.02/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.978800</td>\n",
              "      <td>1.319638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.197300</td>\n",
              "      <td>1.244419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.180600</td>\n",
              "      <td>1.215166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.174200</td>\n",
              "      <td>1.196922</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='314' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [314/351 02:27 < 00:17, 2.12 it/s]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    # compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0d225f4df3194a859266bf18c80176a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_68bb44b537874f38a05c4e12bf25e628",
              "IPY_MODEL_f10618b640554bdbbce4edc2ae43b8a5",
              "IPY_MODEL_101d870bf3f740abb8e966d0519caf60"
            ],
            "layout": "IPY_MODEL_359a7a4fc0684f26aade5d8ca598f21c"
          }
        },
        "68bb44b537874f38a05c4e12bf25e628": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_552f806a85cf4b59805f41c179d294e1",
            "placeholder": "​",
            "style": "IPY_MODEL_2d0bd3532f944acfa475058d676f80a1",
            "value": "Map:  15%"
          }
        },
        "f10618b640554bdbbce4edc2ae43b8a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_545cc548357b44fe8386d974d789f55f",
            "max": 53342,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_78ece4b4d7fc4796bb9846164a833a7c",
            "value": 8000
          }
        },
        "101d870bf3f740abb8e966d0519caf60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7547452333bb4b9c971a2415e9f925c6",
            "placeholder": "​",
            "style": "IPY_MODEL_87a95d64d85e4ab8b697239e0fc79a6b",
            "value": " 8000/53342 [00:47&lt;03:25, 220.74 examples/s]"
          }
        },
        "359a7a4fc0684f26aade5d8ca598f21c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "552f806a85cf4b59805f41c179d294e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d0bd3532f944acfa475058d676f80a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "545cc548357b44fe8386d974d789f55f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78ece4b4d7fc4796bb9846164a833a7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7547452333bb4b9c971a2415e9f925c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87a95d64d85e4ab8b697239e0fc79a6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}